{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prep"
      ],
      "metadata": {
        "id": "wPnLgxrfb97p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hampel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOImGDxnrI4_",
        "outputId": "1f6718c0-21b6-4ba7-9787-c9552115a7e6"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hampel\n",
            "  Downloading hampel-1.0.2.tar.gz (78 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/78.7 kB\u001b[0m \u001b[31m738.2 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m71.7/78.7 kB\u001b[0m \u001b[31m939.4 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m935.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from hampel) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from hampel) (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->hampel) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->hampel) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->hampel) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->hampel) (1.16.0)\n",
            "Building wheels for collected packages: hampel\n",
            "  Building wheel for hampel (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hampel: filename=hampel-1.0.2-cp310-cp310-linux_x86_64.whl size=209227 sha256=a557da93621e40fc5f22f4329ce095ced220e7fe3210fefb89580aa49bf4212c\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/c3/3c/8a9f55c3de0b09faf919393d4c6f09b11b7421dcaa7243b820\n",
            "Successfully built hampel\n",
            "Installing collected packages: hampel\n",
            "Successfully installed hampel-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "BRlBoa6QZXGq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "from scipy.signal import savgol_filter\n",
        "from hampel import hampel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/drive/MyDrive/01.School_related/DoAnChuyenNganh/dataset/segments'\n",
        "\n",
        "# List all files in the folder\n",
        "file_list = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
        "dfs = []\n",
        "labels = []\n",
        "for file_name in file_list:\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    df = pd.read_csv(file_path)\n",
        "    if(len(df) < 600): # Skip all df with less than 600 rows\n",
        "      continue\n",
        "    match = re.search(r\"segment-(\\d+)-(\\d+)\", file_name)\n",
        "    label = match.group(1) # label them based on the first number in the file name\n",
        "    df[\"label\"] = int(label)\n",
        "    dfs.append(df)\n",
        "print(len(dfs))\n",
        "print(len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDmb8HeaZjYQ",
        "outputId": "03833b65-2af8-4c61-8d97-c2b77f56f6ed"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "159\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test model training with raw data"
      ],
      "metadata": {
        "id": "rbm_-sMHbTNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error"
      ],
      "metadata": {
        "id": "AGUA7Y1DcQmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
        "print(merged_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cpe4uYTCfmq8",
        "outputId": "6b72d40e-c6a5-431f-90dc-1ffd1ece1325"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(95400, 53)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_data = pd.DataFrame()\n",
        "for col in merged_df.columns:\n",
        "      col_series = merged_df[col]\n",
        "      # Hampel filter\n",
        "      hampel_filtered = hampel(col_series, window_size=10)\n",
        "      # Savitzky-Golay filter\n",
        "      sg_filtered = savgol_filter(hampel_filtered.filtered_data, window_length=10, polyorder=3)\n",
        "      filtered_data[col] = sg_filtered\n"
      ],
      "metadata": {
        "id": "78vfII4Sq8Yk"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(filtered_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRhsH5kfvkx_",
        "outputId": "ecfe6da9-6ebc-41d5-f608-755cfa5456e3"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(95400, 53)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = filtered_data.drop(\"label\", axis=1)\n",
        "y = filtered_data[\"label\"]\n",
        "\n",
        "print(\"Flattened Features (X):\")\n",
        "print(X.shape)\n",
        "print(\"Labels (y):\")\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "DHLGR_w3c_wg",
        "outputId": "5abf55c8-cbd7-42d1-e300-b60dd0405a87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flattened Features (X):\n",
            "(95400, 52)\n",
            "Labels (y):\n",
            "(95400,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO1BgUijc6R1",
        "outputId": "16a02614-6045-4299-9850-cfb970661e53"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(76320, 52)\n",
            "(76320,)\n",
            "(19080, 52)\n",
            "(19080,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "NqrFyl1NdDgz",
        "outputId": "a7613543-5ccc-4f0f-b90b-f6f905b3436d"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Mean Absolute Error:\", mae)\n",
        "print(\"Mean Squared Error:\", mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzwMF31kdMr9",
        "outputId": "f920e558-5ecb-4310-b5e1-6cba323a15a0"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error: 1.3764989182197815\n",
            "Mean Squared Error: 2.4414722270297737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "yb1gQcYxlqtt"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(1000, activation='relu', input_shape=(X.shape[1],)))\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "opt = Adam(learning_rate=0.0001)\n",
        "\n",
        "model.compile(optimizer=opt,\n",
        "              loss='mse',\n",
        "              metrics=['mae'])"
      ],
      "metadata": {
        "id": "8LK7CIholtZe"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# early stopping setting\n",
        "early_stopping =  EarlyStopping(\n",
        "                            monitor='val_mae',\n",
        "                            min_delta=0,\n",
        "                            patience=10,\n",
        "                            verbose=1,\n",
        "                            mode='auto')"
      ],
      "metadata": {
        "id": "5zldyaT7pB8C"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit\n",
        "history = model.fit(X_train, y_train,\n",
        "                      batch_size=100,\n",
        "                      epochs=200,\n",
        "                      verbose=1,\n",
        "                      validation_split=0.2,\n",
        "                    callbacks=(early_stopping))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onvk6Rttl_zZ",
        "outputId": "9fb423f7-1768-47b3-b09b-040e71bacfb9"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "611/611 [==============================] - 14s 21ms/step - loss: 2.0998 - mae: 1.1926 - val_loss: 1.5526 - val_mae: 0.9895\n",
            "Epoch 2/200\n",
            "611/611 [==============================] - 10s 17ms/step - loss: 1.4062 - mae: 0.9118 - val_loss: 1.2318 - val_mae: 0.8584\n",
            "Epoch 3/200\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 1.1929 - mae: 0.8206 - val_loss: 1.5406 - val_mae: 0.9594\n",
            "Epoch 4/200\n",
            "611/611 [==============================] - 13s 21ms/step - loss: 1.1276 - mae: 0.7873 - val_loss: 1.0447 - val_mae: 0.7724\n",
            "Epoch 5/200\n",
            "611/611 [==============================] - 14s 23ms/step - loss: 1.0428 - mae: 0.7509 - val_loss: 1.1828 - val_mae: 0.7863\n",
            "Epoch 6/200\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.9572 - mae: 0.7110 - val_loss: 1.1441 - val_mae: 0.7953\n",
            "Epoch 7/200\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.9198 - mae: 0.6924 - val_loss: 0.8588 - val_mae: 0.6639\n",
            "Epoch 8/200\n",
            "611/611 [==============================] - 13s 21ms/step - loss: 0.8818 - mae: 0.6739 - val_loss: 0.8289 - val_mae: 0.6498\n",
            "Epoch 9/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.8384 - mae: 0.6556 - val_loss: 0.9083 - val_mae: 0.6761\n",
            "Epoch 10/200\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.8436 - mae: 0.6551 - val_loss: 0.8832 - val_mae: 0.6719\n",
            "Epoch 11/200\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.7801 - mae: 0.6251 - val_loss: 0.7679 - val_mae: 0.6123\n",
            "Epoch 12/200\n",
            "611/611 [==============================] - 13s 21ms/step - loss: 0.7675 - mae: 0.6190 - val_loss: 0.8698 - val_mae: 0.6903\n",
            "Epoch 13/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.7378 - mae: 0.6053 - val_loss: 0.6999 - val_mae: 0.5865\n",
            "Epoch 14/200\n",
            "611/611 [==============================] - 10s 17ms/step - loss: 0.7406 - mae: 0.6060 - val_loss: 0.8482 - val_mae: 0.6586\n",
            "Epoch 15/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.7116 - mae: 0.5912 - val_loss: 0.8063 - val_mae: 0.6371\n",
            "Epoch 16/200\n",
            "611/611 [==============================] - 13s 21ms/step - loss: 0.6918 - mae: 0.5799 - val_loss: 0.7115 - val_mae: 0.5921\n",
            "Epoch 17/200\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.6859 - mae: 0.5769 - val_loss: 0.7296 - val_mae: 0.6116\n",
            "Epoch 18/200\n",
            "611/611 [==============================] - 11s 17ms/step - loss: 0.6772 - mae: 0.5716 - val_loss: 0.7341 - val_mae: 0.5710\n",
            "Epoch 19/200\n",
            "611/611 [==============================] - 13s 20ms/step - loss: 0.6466 - mae: 0.5556 - val_loss: 0.6147 - val_mae: 0.5320\n",
            "Epoch 20/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.6441 - mae: 0.5548 - val_loss: 0.6045 - val_mae: 0.5365\n",
            "Epoch 21/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.6276 - mae: 0.5451 - val_loss: 0.7118 - val_mae: 0.5737\n",
            "Epoch 22/200\n",
            "611/611 [==============================] - 10s 17ms/step - loss: 0.6030 - mae: 0.5333 - val_loss: 0.7480 - val_mae: 0.6007\n",
            "Epoch 23/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.6108 - mae: 0.5365 - val_loss: 0.6365 - val_mae: 0.5380\n",
            "Epoch 24/200\n",
            "611/611 [==============================] - 13s 21ms/step - loss: 0.5774 - mae: 0.5190 - val_loss: 0.8188 - val_mae: 0.6058\n",
            "Epoch 25/200\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.5774 - mae: 0.5195 - val_loss: 0.5703 - val_mae: 0.5102\n",
            "Epoch 26/200\n",
            "611/611 [==============================] - 11s 17ms/step - loss: 0.5673 - mae: 0.5126 - val_loss: 0.6166 - val_mae: 0.5743\n",
            "Epoch 27/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.5756 - mae: 0.5170 - val_loss: 0.6093 - val_mae: 0.5425\n",
            "Epoch 28/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.5499 - mae: 0.5031 - val_loss: 0.5869 - val_mae: 0.5400\n",
            "Epoch 29/200\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.5432 - mae: 0.4996 - val_loss: 0.5526 - val_mae: 0.4998\n",
            "Epoch 30/200\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.5312 - mae: 0.4941 - val_loss: 0.5541 - val_mae: 0.4887\n",
            "Epoch 31/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.5375 - mae: 0.4947 - val_loss: 0.5343 - val_mae: 0.5091\n",
            "Epoch 32/200\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.5105 - mae: 0.4825 - val_loss: 0.5396 - val_mae: 0.4789\n",
            "Epoch 33/200\n",
            "611/611 [==============================] - 11s 17ms/step - loss: 0.5217 - mae: 0.4881 - val_loss: 0.5069 - val_mae: 0.4668\n",
            "Epoch 34/200\n",
            "611/611 [==============================] - 13s 21ms/step - loss: 0.5017 - mae: 0.4790 - val_loss: 0.5395 - val_mae: 0.4952\n",
            "Epoch 35/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.5065 - mae: 0.4806 - val_loss: 0.5490 - val_mae: 0.4949\n",
            "Epoch 36/200\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.5009 - mae: 0.4756 - val_loss: 0.5059 - val_mae: 0.4781\n",
            "Epoch 37/200\n",
            "611/611 [==============================] - 11s 17ms/step - loss: 0.4909 - mae: 0.4711 - val_loss: 0.5530 - val_mae: 0.4722\n",
            "Epoch 38/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.4796 - mae: 0.4654 - val_loss: 0.4833 - val_mae: 0.4467\n",
            "Epoch 39/200\n",
            "611/611 [==============================] - 13s 21ms/step - loss: 0.4671 - mae: 0.4615 - val_loss: 0.5295 - val_mae: 0.5111\n",
            "Epoch 40/200\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.4714 - mae: 0.4658 - val_loss: 0.5308 - val_mae: 0.4733\n",
            "Epoch 41/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.4581 - mae: 0.4548 - val_loss: 0.4756 - val_mae: 0.4593\n",
            "Epoch 42/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.4553 - mae: 0.4524 - val_loss: 0.4874 - val_mae: 0.4612\n",
            "Epoch 43/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.4593 - mae: 0.4533 - val_loss: 0.5864 - val_mae: 0.5233\n",
            "Epoch 44/200\n",
            "611/611 [==============================] - 10s 16ms/step - loss: 0.4607 - mae: 0.4563 - val_loss: 0.5692 - val_mae: 0.5022\n",
            "Epoch 45/200\n",
            "611/611 [==============================] - 13s 21ms/step - loss: 0.4417 - mae: 0.4442 - val_loss: 0.4734 - val_mae: 0.4604\n",
            "Epoch 46/200\n",
            "611/611 [==============================] - 13s 21ms/step - loss: 0.4280 - mae: 0.4391 - val_loss: 0.4810 - val_mae: 0.4606\n",
            "Epoch 47/200\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.4322 - mae: 0.4398 - val_loss: 0.4574 - val_mae: 0.4388\n",
            "Epoch 48/200\n",
            "611/611 [==============================] - 11s 17ms/step - loss: 0.4250 - mae: 0.4385 - val_loss: 0.4453 - val_mae: 0.4353\n",
            "Epoch 49/200\n",
            "611/611 [==============================] - 13s 21ms/step - loss: 0.4258 - mae: 0.4370 - val_loss: 0.5704 - val_mae: 0.4751\n",
            "Epoch 50/200\n",
            "611/611 [==============================] - 13s 21ms/step - loss: 0.4196 - mae: 0.4326 - val_loss: 0.6697 - val_mae: 0.5235\n",
            "Epoch 51/200\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.4034 - mae: 0.4240 - val_loss: 0.4567 - val_mae: 0.4868\n",
            "Epoch 52/200\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.4004 - mae: 0.4252 - val_loss: 0.4919 - val_mae: 0.4598\n",
            "Epoch 53/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.4013 - mae: 0.4235 - val_loss: 0.5371 - val_mae: 0.5305\n",
            "Epoch 54/200\n",
            "611/611 [==============================] - 13s 21ms/step - loss: 0.3978 - mae: 0.4212 - val_loss: 0.4330 - val_mae: 0.4378\n",
            "Epoch 55/200\n",
            "611/611 [==============================] - 11s 19ms/step - loss: 0.3952 - mae: 0.4217 - val_loss: 0.4839 - val_mae: 0.4708\n",
            "Epoch 56/200\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.3937 - mae: 0.4190 - val_loss: 0.4392 - val_mae: 0.4347\n",
            "Epoch 57/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.3679 - mae: 0.4054 - val_loss: 0.4233 - val_mae: 0.4234\n",
            "Epoch 58/200\n",
            "611/611 [==============================] - 13s 21ms/step - loss: 0.3815 - mae: 0.4151 - val_loss: 0.4410 - val_mae: 0.4488\n",
            "Epoch 59/200\n",
            "611/611 [==============================] - 11s 18ms/step - loss: 0.3810 - mae: 0.4118 - val_loss: 0.4370 - val_mae: 0.4383\n",
            "Epoch 60/200\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.3618 - mae: 0.4023 - val_loss: 0.4016 - val_mae: 0.4329\n",
            "Epoch 61/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.3726 - mae: 0.4111 - val_loss: 0.3921 - val_mae: 0.4113\n",
            "Epoch 62/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.3503 - mae: 0.3960 - val_loss: 0.4936 - val_mae: 0.4969\n",
            "Epoch 63/200\n",
            "611/611 [==============================] - 10s 17ms/step - loss: 0.3519 - mae: 0.3967 - val_loss: 0.5018 - val_mae: 0.4450\n",
            "Epoch 64/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.3542 - mae: 0.4002 - val_loss: 0.3897 - val_mae: 0.4088\n",
            "Epoch 65/200\n",
            "611/611 [==============================] - 13s 21ms/step - loss: 0.3508 - mae: 0.3948 - val_loss: 0.4292 - val_mae: 0.4293\n",
            "Epoch 66/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.3509 - mae: 0.3958 - val_loss: 0.4156 - val_mae: 0.4357\n",
            "Epoch 67/200\n",
            "611/611 [==============================] - 10s 16ms/step - loss: 0.3342 - mae: 0.3861 - val_loss: 0.4305 - val_mae: 0.4341\n",
            "Epoch 68/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.3368 - mae: 0.3881 - val_loss: 0.3715 - val_mae: 0.4171\n",
            "Epoch 69/200\n",
            "611/611 [==============================] - 13s 21ms/step - loss: 0.3236 - mae: 0.3809 - val_loss: 0.4010 - val_mae: 0.4208\n",
            "Epoch 70/200\n",
            "611/611 [==============================] - 11s 19ms/step - loss: 0.3383 - mae: 0.3875 - val_loss: 0.4199 - val_mae: 0.4191\n",
            "Epoch 71/200\n",
            "611/611 [==============================] - 11s 19ms/step - loss: 0.3262 - mae: 0.3811 - val_loss: 0.3762 - val_mae: 0.3994\n",
            "Epoch 72/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.3234 - mae: 0.3830 - val_loss: 0.3597 - val_mae: 0.3904\n",
            "Epoch 73/200\n",
            "611/611 [==============================] - 13s 21ms/step - loss: 0.3229 - mae: 0.3799 - val_loss: 0.4469 - val_mae: 0.4305\n",
            "Epoch 74/200\n",
            "611/611 [==============================] - 11s 17ms/step - loss: 0.3059 - mae: 0.3710 - val_loss: 0.3592 - val_mae: 0.4071\n",
            "Epoch 75/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.3117 - mae: 0.3731 - val_loss: 0.3595 - val_mae: 0.3965\n",
            "Epoch 76/200\n",
            "611/611 [==============================] - 13s 21ms/step - loss: 0.3262 - mae: 0.3832 - val_loss: 0.3462 - val_mae: 0.3858\n",
            "Epoch 77/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.3238 - mae: 0.3797 - val_loss: 0.3616 - val_mae: 0.3956\n",
            "Epoch 78/200\n",
            "611/611 [==============================] - 10s 17ms/step - loss: 0.2991 - mae: 0.3680 - val_loss: 0.3601 - val_mae: 0.3987\n",
            "Epoch 79/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.2936 - mae: 0.3650 - val_loss: 0.3466 - val_mae: 0.3790\n",
            "Epoch 80/200\n",
            "611/611 [==============================] - 13s 21ms/step - loss: 0.2883 - mae: 0.3588 - val_loss: 0.3466 - val_mae: 0.3903\n",
            "Epoch 81/200\n",
            "611/611 [==============================] - 13s 21ms/step - loss: 0.3042 - mae: 0.3702 - val_loss: 0.3944 - val_mae: 0.4159\n",
            "Epoch 82/200\n",
            "611/611 [==============================] - 11s 17ms/step - loss: 0.2980 - mae: 0.3659 - val_loss: 0.3872 - val_mae: 0.4129\n",
            "Epoch 83/200\n",
            "611/611 [==============================] - 12s 19ms/step - loss: 0.2873 - mae: 0.3620 - val_loss: 0.3656 - val_mae: 0.3978\n",
            "Epoch 84/200\n",
            "611/611 [==============================] - 13s 21ms/step - loss: 0.2840 - mae: 0.3610 - val_loss: 0.4089 - val_mae: 0.4232\n",
            "Epoch 85/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.2870 - mae: 0.3608 - val_loss: 0.3733 - val_mae: 0.4068\n",
            "Epoch 86/200\n",
            "611/611 [==============================] - 11s 17ms/step - loss: 0.2826 - mae: 0.3578 - val_loss: 0.3615 - val_mae: 0.3975\n",
            "Epoch 87/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.2744 - mae: 0.3535 - val_loss: 0.3793 - val_mae: 0.4045\n",
            "Epoch 88/200\n",
            "611/611 [==============================] - 13s 21ms/step - loss: 0.2886 - mae: 0.3637 - val_loss: 0.3672 - val_mae: 0.4268\n",
            "Epoch 89/200\n",
            "611/611 [==============================] - 12s 20ms/step - loss: 0.2784 - mae: 0.3571 - val_loss: 0.3569 - val_mae: 0.3893\n",
            "Epoch 89: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        " # Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Calculate Mean Absolute Error\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"Mean Absolute Error:\", mae)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-gvUcF00kJ3",
        "outputId": "7771b961-c7b0-4b75-e68f-8cea675d2e89"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "597/597 [==============================] - 3s 5ms/step\n",
            "Mean Squared Error: 0.33261386\n",
            "Mean Absolute Error: 0.37751868\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predicted values (y_pred) | Actual values (y_test)\")\n",
        "for pred, actual in zip(y_pred[:20], y_test[:20]):\n",
        "    print(\"{:.2f}                     | {:.2f}\".format(pred[0], actual))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RLyUH451j3x",
        "outputId": "b0bc4c9f-3bfe-4912-a4e7-d85a71150413"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted values (y_pred) | Actual values (y_test)\n",
            "0.70                     | 1.00\n",
            "5.11                     | 5.00\n",
            "0.41                     | 1.00\n",
            "2.02                     | 2.00\n",
            "2.01                     | 2.00\n",
            "3.59                     | 3.00\n",
            "2.59                     | 2.00\n",
            "0.42                     | 0.00\n",
            "4.32                     | 4.00\n",
            "2.07                     | 2.00\n",
            "0.64                     | 0.00\n",
            "0.55                     | 1.00\n",
            "1.95                     | 2.00\n",
            "0.98                     | 0.00\n",
            "2.77                     | 3.00\n",
            "0.62                     | 1.00\n",
            "3.77                     | 4.00\n",
            "0.65                     | 0.00\n",
            "4.35                     | 4.00\n",
            "3.87                     | 0.00\n"
          ]
        }
      ]
    }
  ]
}